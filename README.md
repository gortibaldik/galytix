# Assignment

## How to Run

At first, the data should be put into `./data`. For some reason, anytime I tried to use `gdown` for downloading vectors data from google, I got the exception that too many users try to download this data. The code tries to download the file, however if it fails then the application won't start.

The `./data` should contain files specified in [`config.py`](./embedding_engine/config.py).

The solution should be runnable with `docker compose`, please, use the following command to run it:

```
sudo docker compose up --build
```

Currently, the only frontend that is available out of the box is the swagger generated by FastAPI. Navigate to `localhost:5000/docs`, and call the `/nearest` endpoint. The result contains 2 fields, `phrase` and `dist`.

## Simple Configuration



## TODO

I spent too much time unsuccessfuly debuging automatic download of GoogleNews vectors, and then focused too much on using Postgresql database. As of now, the database works, all the word vectors are loaded to the database and the search for word is offloaded to this source. However, regarding the phrases, I store them only in simple python arrays because I do not see any advantage of using more complicated data structures such as pandas/polars.

- [ ] Automatic download of GoogleNews-vectors
- [ ] Threaded execution of extractions to vectors.csv to see the progress
- [ ] Right now the data pipeline is very rigid, not configurable, the individual processing steps are cached to `vectors.csv` and then to `vectors-processed.csv`, the ideal solution would be a pipeline which would remove the need for intermediate files
- [x] I would like to add some form of configuration, so that either the distance calculation (l2 or cosine) would be configurable from the environment variables
- [ ] there are no tests, and I'm not sure that my code actually works. Even if I had tested it on a few cases, there are probably still some bugs, maybe even critical bugs.
- [ ] more documentation, currently I added docstrings only on places where I did not forget to
